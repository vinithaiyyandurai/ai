# -*- coding: utf-8 -*-
"""Untitled61.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_lQ6ZZ9nXxsqBceg7SXO1VjcE5Gu1cRE
"""

#!/usr/bin/env python3
"""
advanced_time_series_attention.py
Single-file implementation of:
"Advanced Time Series Forecasting with Deep Learning and Attention Mechanisms"

- Generates multivariate synthetic dataset (consumption + temperature)
- Builds baseline LSTM and Attention-LSTM (additive attention)
- Trains with EarlyStopping and ModelCheckpoint
- Evaluates RMSE / MAE / MAPE (on original scale)
- Visualizes predictions and attention weights
- Saves models and a JSON config

Run: python advanced_time_series_attention.py
(or paste entire file into a single Google Colab cell and run)
"""

import os
import sys
import json
import math
import subprocess
from pathlib import Path

# Try to install required packages if missing (useful in Colab)
required = ["tensorflow", "numpy", "pandas", "matplotlib", "scikit-learn"]
try:
    import tensorflow as tf
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.metrics import mean_squared_error, mean_absolute_error
except Exception as e:
    print("Missing packages detected, attempting pip install. Re-run the script if kernel restarts.")
    subprocess.check_call([sys.executable, "-m", "pip", "install", *required])
    import tensorflow as tf
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.metrics import mean_squared_error, mean_absolute_error

# Paths to screenshots from this conversation (developer-supplied)
screenshot_1 = "/mnt/data/Screenshot (26).png"
screenshot_2 = "/mnt/data/Screenshot (27).png"

# Create output/model folder
os.makedirs("models", exist_ok=True)
os.makedirs("plots", exist_ok=True)

# Reproducibility
np.random.seed(42)
tf.random.set_seed(42)

def generate_synthetic(days=1800):
    t = np.arange(days)
    daily = 8 * np.sin(2 * np.pi * t / 7)         # weekly-like (7-day)
    weekly = 4 * np.sin(2 * np.pi * t / 30)       # monthly-ish
    yearly = 3 * np.sin(2 * np.pi * t / 365)      # yearly cycle
    trend = 0.01 * t
    noise = np.random.normal(0, 1.0, days)

    consumption = 50 + daily + weekly + yearly + trend + noise
    temperature = 20 + 10 * np.sin(2 * np.pi * t / 365) + np.random.normal(0, 0.8, days)

    df = pd.DataFrame({
        "consumption": consumption,
        "temperature": temperature
    }, index=pd.date_range("2015-01-01", periods=days, freq="D"))
    return df

def create_windows(scaled_features, scaled_target, time_steps=30):
    Xs, ys = [], []
    N = len(scaled_features)
    for i in range(N - time_steps):
        Xs.append(scaled_features[i:i+time_steps])
        ys.append(scaled_target[i+time_steps, 0])
    return np.array(Xs), np.array(ys)

# Additive Time Attention Layer
from tensorflow.keras import layers, Model, Input

class TimeAttention(layers.Layer):
    def __init__(self, units=None, **kwargs):
        super(TimeAttention, self).__init__(**kwargs)
        self.units = units

    def build(self, input_shape):
        # input_shape: (batch, time, features)
        _, time, features = input_shape
        if self.units is None:
            self.units = features
        self.W1 = self.add_weight(shape=(features, self.units),
                                  initializer='glorot_uniform',
                                  trainable=True, name='W1')
        self.W2 = self.add_weight(shape=(self.units, 1),
                                  initializer='glorot_uniform',
                                  trainable=True, name='W2')
        self.b = self.add_weight(shape=(self.units,),
                                 initializer='zeros', trainable=True, name='b')
        super(TimeAttention, self).build(input_shape)

    def call(self, inputs):
        # inputs: (batch, time, features)
        u = tf.tensordot(inputs, self.W1, axes=[[2],[0]])  # (batch, time, units)
        u = tf.nn.tanh(u + self.b)
        scores = tf.tensordot(u, self.W2, axes=[[2],[0]])  # (batch, time, 1)
        scores = tf.squeeze(scores, axis=-1)  # (batch, time)
        alpha = tf.nn.softmax(scores, axis=1)
        alpha_expanded = tf.expand_dims(alpha, axis=-1)
        context = tf.reduce_sum(inputs * alpha_expanded, axis=1)
        return context, alpha

def build_baseline_lstm(input_shape, lstm_units=64):
    inp = Input(shape=input_shape)
    x = layers.LSTM(lstm_units, return_sequences=False)(inp)
    x = layers.Dense(32, activation='relu')(x)
    out = layers.Dense(1, activation='linear')(x)
    model = Model(inputs=inp, outputs=out)
    model.compile(optimizer='adam', loss='mse')
    return model

def build_attention_lstm(input_shape, lstm_units=64, attention_units=None):
    inp = Input(shape=input_shape, name='input_seq')
    x = layers.LSTM(lstm_units, return_sequences=True, name='lstm_seq')(inp)
    attention_layer = TimeAttention(units=attention_units)
    context, att_weights = attention_layer(x)
    dense = layers.Dense(32, activation='relu')(context)
    out = layers.Dense(1, activation='linear', name='prediction')(dense)
    model = Model(inputs=inp, outputs=out)
    att_model = Model(inputs=inp, outputs=att_weights)
    model.compile(optimizer='adam', loss='mse')
    return model, att_model

def inverse_transform_scaled(scaler, arr):
    # expects arr shape (n,1)
    return scaler.inverse_transform(arr).reshape(-1)

def metrics(y_true, y_pred):
    rmse = math.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    eps = 1e-8
    mape = float(np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + eps))) * 100)
    return rmse, mae, mape

def main():
    print("1) Generating synthetic data...")
    df = generate_synthetic(days=1800)
    print("Data shape:", df.shape)
    # Quick sanity plot saved
    plt.figure(figsize=(12,3))
    plt.plot(df['consumption'][:400])
    plt.title("Sample consumption (first 400 days)")
    plt.tight_layout()
    plt.savefig("plots/consumption_sample.png")
    plt.close()

    # Preprocess / scale
    feature_cols = ['consumption','temperature']
    target_col = 'consumption'
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()

    X_all = df[feature_cols].values
    X_scaled = scaler_X.fit_transform(X_all)
    y_all = df[[target_col]].values
    y_scaled_all = scaler_y.fit_transform(y_all)

    # Windowing
    time_steps = 30
    X, y = create_windows(X_scaled, y_scaled_all, time_steps=time_steps)
    split = int(0.8 * len(X))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]
    print("Shapes -> X_train:", X_train.shape, "X_test:", X_test.shape)

    # Build baseline LSTM
    print("2) Building baseline LSTM...")
    baseline = build_baseline_lstm((time_steps, X.shape[2]), lstm_units=64)
    baseline.summary()

    # Train baseline
    print("3) Training baseline LSTM...")
    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)
    ckpt_path_baseline = "models/baseline_best.h5"
    ckptb = tf.keras.callbacks.ModelCheckpoint(ckpt_path_baseline, monitor='val_loss', save_best_only=True)
    history_baseline = baseline.fit(
        X_train, y_train,
        validation_split=0.2,
        epochs=50,
        batch_size=32,
        callbacks=[es, ckptb],
        verbose=1
    )

    # Build attention LSTM
    print("4) Building Attention-LSTM...")
    att_model, att_w_model = build_attention_lstm((time_steps, X.shape[2]), lstm_units=64, attention_units=32)
    att_model.summary()

    # Train attention model
    print("5) Training Attention-LSTM...")
    es2 = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)
    ckpt_path_att = "models/att_lstm_best.h5"
    ckpta = tf.keras.callbacks.ModelCheckpoint(ckpt_path_att, monitor='val_loss', save_best_only=True)
    history_att = att_model.fit(
        X_train, y_train,
        validation_split=0.2,
        epochs=80,
        batch_size=32,
        callbacks=[es2, ckpta],
        verbose=1
    )

    # Predict (scaled) and inverse transform
    print("6) Predictions and inverse-scaling...")
    y_pred_baseline_scaled = baseline.predict(X_test).reshape(-1,1)
    y_pred_att_scaled = att_model.predict(X_test).reshape(-1,1)
    y_test_scaled = y_test.reshape(-1,1)

    y_pred_baseline = inverse_transform_scaled(scaler_y, y_pred_baseline_scaled)
    y_pred_att = inverse_transform_scaled(scaler_y, y_pred_att_scaled)
    y_true = inverse_transform_scaled(scaler_y, y_test_scaled)

    # Metrics
    baseline_rmse, baseline_mae, baseline_mape = metrics(y_true, y_pred_baseline)
    att_rmse, att_mae, att_mape = metrics(y_true, y_pred_att)

    print("\n=== Evaluation (original scale) ===")
    print(f"Baseline LSTM  -> RMSE: {baseline_rmse:.4f}, MAE: {baseline_mae:.4f}, MAPE: {baseline_mape:.4f}%")
    print(f"Attention LSTM -> RMSE: {att_rmse:.4f}, MAE: {att_mae:.4f}, MAPE: {att_mape:.4f}%")

    # Save metrics & config
    config = {
        "time_steps": time_steps,
        "features": feature_cols,
        "baseline": {"lstm_units": 64, "dense_after": [32,1]},
        "attention_model": {"lstm_units": 64, "attention_units": 32},
        "metrics": {
            "baseline": {"rmse": baseline_rmse, "mae": baseline_mae, "mape": baseline_mape},
            "attention": {"rmse": att_rmse, "mae": att_mae, "mape": att_mape}
        },
        "screenshots": [screenshot_1, screenshot_2]
    }
    with open("models/model_config.json", "w") as f:
        json.dump(config, f, indent=2)
    print("Saved model_config.json to models/")

    # Save models
    try:
        baseline.save("models/baseline_model_final.h5")
        att_model.save("models/att_lstm_model_final.h5")
        print("Saved models to models/")
    except Exception as e:
        print("Model save failed (maybe HDF5 vs SavedModel format); continuing. Error:", e)

    # Plot a sample of actual vs predicted
    print("7) Saving prediction plot...")
    nplot = min(300, len(y_true))
    plt.figure(figsize=(14,5))
    plt.plot(y_true[:nplot], label='Actual', linewidth=1.2)
    plt.plot(y_pred_baseline[:nplot], label='Baseline LSTM', linewidth=1)
    plt.plot(y_pred_att[:nplot], label='Attention LSTM', linewidth=1)
    plt.legend()
    plt.title("Actual vs Predicted (sample)")
    plt.tight_layout()
    plt.savefig("plots/predictions_comparison.png")
    plt.close()

    # Extract attention weights for test set and plot
    print("8) Extracting and saving attention visualizations...")
    att_weights_test = att_w_model.predict(X_test)  # (n_samples, time_steps)
    np.save("models/att_weights_test.npy", att_weights_test)

    # Average attention across test samples
    avg_att = np.mean(att_weights_test, axis=0)
    plt.figure(figsize=(10,4))
    plt.bar(np.arange(time_steps), avg_att)
    plt.xlabel("Lag index (0 = oldest in window)")
    plt.ylabel("Average attention weight")
    plt.title("Average attention weight across test samples")
    plt.tight_layout()
    plt.savefig("plots/avg_attention.png")
    plt.close()

    # Heatmap for first few test samples
    n_display = min(6, att_weights_test.shape[0])
    fig, axes = plt.subplots(n_display, 1, figsize=(12, 2.2 * n_display))
    for i in range(n_display):
        ax = axes[i] if n_display > 1 else axes
        im = ax.imshow(att_weights_test[i:i+1], aspect='auto')
        ax.set_yticks([])
        ax.set_xticks(np.arange(time_steps))
        ax.set_title(f"Attention weights sample {i}")
    fig.colorbar(im, ax=axes.ravel().tolist(), orientation='horizontal', fraction=0.02)
    plt.tight_layout()
    plt.savefig("plots/attention_heatmaps.png")
    plt.close()

    print("Plots & attention saved in ./plots/. Models & config in ./models/")
    print("Screenshots referenced (local paths):")
    print(" -", screenshot_1)
    print(" -", screenshot_2)
    print("\nDone.")

if __name__ == "__main__":
    main()